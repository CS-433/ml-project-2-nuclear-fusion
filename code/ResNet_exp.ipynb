{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Kaan-wq/ml_tokamak/blob/main/NN_general.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wnJP6X6bVhZQ",
    "outputId": "30fb24eb-636d-4f95-f7c5-402321f67a15"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as mpatches\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from scipy.fft import fft, ifft\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import keras\n",
    "import keras_tuner as kt\n",
    "from kerastuner import RandomSearch\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**I - Preprocessing of the data**\n",
    "\n",
    "Below is the preprocessing pipeline of the data. \\\\\n",
    "Essentially, we feature engineer a few columns, normalize the data and finally, we split it into a training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "louv_dDDVk0h"
   },
   "outputs": [],
   "source": [
    "# load dataset using pickle\n",
    "import pickle\n",
    "with open(\"../data/dataset_disruption_characterization.pickle\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ME0mg-qbzWRC"
   },
   "source": [
    "Here we load the data from the drive and put it into a more practical **data structure**. \\\\\n",
    "We add a column <code>['IPE']</code> which represents the current difference between the reference and actual currents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "xhsprZ_wi8-9",
    "outputId": "02ca27a5-c7ee-46b8-b5b2-6f7abc478f1f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IPLA</th>\n",
       "      <th>IPref</th>\n",
       "      <th>ECEcore</th>\n",
       "      <th>SSXcore</th>\n",
       "      <th>LI</th>\n",
       "      <th>Q95</th>\n",
       "      <th>ZMAG</th>\n",
       "      <th>Vloop</th>\n",
       "      <th>IPE</th>\n",
       "      <th>IPLA_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Vloop_der</th>\n",
       "      <th>Vloop_der2</th>\n",
       "      <th>Vloop_acf</th>\n",
       "      <th>Vloop_pacf</th>\n",
       "      <th>Time</th>\n",
       "      <th>Frame</th>\n",
       "      <th>Event</th>\n",
       "      <th>Label</th>\n",
       "      <th>Shot</th>\n",
       "      <th>Window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1989456.750</td>\n",
       "      <td>1999500.0</td>\n",
       "      <td>1740.929077</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>1.191489</td>\n",
       "      <td>3.874169</td>\n",
       "      <td>0.30388</td>\n",
       "      <td>-0.519496</td>\n",
       "      <td>10043.250</td>\n",
       "      <td>1989371.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.238922</td>\n",
       "      <td>-0.019993</td>\n",
       "      <td>0.034098</td>\n",
       "      <td>-0.074084</td>\n",
       "      <td>10.361</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>81206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1989606.250</td>\n",
       "      <td>1999500.0</td>\n",
       "      <td>1744.737427</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>1.191489</td>\n",
       "      <td>3.874169</td>\n",
       "      <td>0.30388</td>\n",
       "      <td>-0.758418</td>\n",
       "      <td>9893.750</td>\n",
       "      <td>1989371.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258915</td>\n",
       "      <td>0.119461</td>\n",
       "      <td>0.034098</td>\n",
       "      <td>-0.074084</td>\n",
       "      <td>10.362</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>81206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988484.000</td>\n",
       "      <td>1999500.0</td>\n",
       "      <td>1756.823730</td>\n",
       "      <td>0.008698</td>\n",
       "      <td>1.191489</td>\n",
       "      <td>3.874169</td>\n",
       "      <td>0.30388</td>\n",
       "      <td>-1.037327</td>\n",
       "      <td>11016.000</td>\n",
       "      <td>1989371.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448103</td>\n",
       "      <td>0.034098</td>\n",
       "      <td>-0.074084</td>\n",
       "      <td>10.363</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>81206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1989329.625</td>\n",
       "      <td>1999500.0</td>\n",
       "      <td>1756.823730</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>1.191489</td>\n",
       "      <td>3.874169</td>\n",
       "      <td>0.30388</td>\n",
       "      <td>-0.758418</td>\n",
       "      <td>10170.375</td>\n",
       "      <td>1989371.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637292</td>\n",
       "      <td>-0.159448</td>\n",
       "      <td>0.034098</td>\n",
       "      <td>-0.074084</td>\n",
       "      <td>10.364</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>81206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990532.250</td>\n",
       "      <td>1999500.0</td>\n",
       "      <td>1746.057251</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>1.191489</td>\n",
       "      <td>3.874169</td>\n",
       "      <td>0.30388</td>\n",
       "      <td>0.237256</td>\n",
       "      <td>8967.750</td>\n",
       "      <td>1989371.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.318896</td>\n",
       "      <td>-0.537741</td>\n",
       "      <td>0.034098</td>\n",
       "      <td>-0.074084</td>\n",
       "      <td>10.365</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>81206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17975</th>\n",
       "      <td>3993662.500</td>\n",
       "      <td>4000500.0</td>\n",
       "      <td>6282.263672</td>\n",
       "      <td>1.416517</td>\n",
       "      <td>0.808256</td>\n",
       "      <td>2.898196</td>\n",
       "      <td>0.32961</td>\n",
       "      <td>-0.316564</td>\n",
       "      <td>6837.500</td>\n",
       "      <td>3996622.75</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138622</td>\n",
       "      <td>0.138705</td>\n",
       "      <td>0.039632</td>\n",
       "      <td>0.056425</td>\n",
       "      <td>8.622</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98005</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17976</th>\n",
       "      <td>3995614.250</td>\n",
       "      <td>4000500.0</td>\n",
       "      <td>6303.055664</td>\n",
       "      <td>1.410871</td>\n",
       "      <td>0.808256</td>\n",
       "      <td>2.898196</td>\n",
       "      <td>0.32961</td>\n",
       "      <td>-0.237256</td>\n",
       "      <td>4885.750</td>\n",
       "      <td>3996622.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038821</td>\n",
       "      <td>0.198435</td>\n",
       "      <td>0.039632</td>\n",
       "      <td>0.056425</td>\n",
       "      <td>8.623</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98005</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17977</th>\n",
       "      <td>3994348.000</td>\n",
       "      <td>4000500.0</td>\n",
       "      <td>6277.216309</td>\n",
       "      <td>1.402783</td>\n",
       "      <td>0.808256</td>\n",
       "      <td>2.898196</td>\n",
       "      <td>0.32961</td>\n",
       "      <td>-0.238922</td>\n",
       "      <td>6152.000</td>\n",
       "      <td>3996622.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258249</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.039632</td>\n",
       "      <td>0.056425</td>\n",
       "      <td>8.624</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98005</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17978</th>\n",
       "      <td>3997734.500</td>\n",
       "      <td>4000500.0</td>\n",
       "      <td>6286.124512</td>\n",
       "      <td>1.391034</td>\n",
       "      <td>0.808256</td>\n",
       "      <td>2.898196</td>\n",
       "      <td>0.32961</td>\n",
       "      <td>0.279242</td>\n",
       "      <td>2765.500</td>\n",
       "      <td>3996622.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039654</td>\n",
       "      <td>-0.348553</td>\n",
       "      <td>0.039632</td>\n",
       "      <td>0.056425</td>\n",
       "      <td>8.625</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98005</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17979</th>\n",
       "      <td>3995231.500</td>\n",
       "      <td>4000500.0</td>\n",
       "      <td>6317.835449</td>\n",
       "      <td>1.415449</td>\n",
       "      <td>0.808256</td>\n",
       "      <td>2.898196</td>\n",
       "      <td>0.32961</td>\n",
       "      <td>-0.159614</td>\n",
       "      <td>5268.500</td>\n",
       "      <td>3996622.75</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438856</td>\n",
       "      <td>-0.478510</td>\n",
       "      <td>0.039632</td>\n",
       "      <td>0.056425</td>\n",
       "      <td>8.626</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98005</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17980 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              IPLA      IPref      ECEcore   SSXcore        LI       Q95  \\\n",
       "0      1989456.750  1999500.0  1740.929077  0.008850  1.191489  3.874169   \n",
       "1      1989606.250  1999500.0  1744.737427  0.008850  1.191489  3.874169   \n",
       "2      1988484.000  1999500.0  1756.823730  0.008698  1.191489  3.874169   \n",
       "3      1989329.625  1999500.0  1756.823730  0.008469  1.191489  3.874169   \n",
       "4      1990532.250  1999500.0  1746.057251  0.008850  1.191489  3.874169   \n",
       "...            ...        ...          ...       ...       ...       ...   \n",
       "17975  3993662.500  4000500.0  6282.263672  1.416517  0.808256  2.898196   \n",
       "17976  3995614.250  4000500.0  6303.055664  1.410871  0.808256  2.898196   \n",
       "17977  3994348.000  4000500.0  6277.216309  1.402783  0.808256  2.898196   \n",
       "17978  3997734.500  4000500.0  6286.124512  1.391034  0.808256  2.898196   \n",
       "17979  3995231.500  4000500.0  6317.835449  1.415449  0.808256  2.898196   \n",
       "\n",
       "          ZMAG     Vloop        IPE   IPLA_mean  ...  Vloop_der  Vloop_der2  \\\n",
       "0      0.30388 -0.519496  10043.250  1989371.00  ...  -0.238922   -0.019993   \n",
       "1      0.30388 -0.758418   9893.750  1989371.00  ...  -0.258915    0.119461   \n",
       "2      0.30388 -1.037327  11016.000  1989371.00  ...   0.000000    0.448103   \n",
       "3      0.30388 -0.758418  10170.375  1989371.00  ...   0.637292   -0.159448   \n",
       "4      0.30388  0.237256   8967.750  1989371.00  ...  -0.318896   -0.537741   \n",
       "...        ...       ...        ...         ...  ...        ...         ...   \n",
       "17975  0.32961 -0.316564   6837.500  3996622.75  ...  -0.138622    0.138705   \n",
       "17976  0.32961 -0.237256   4885.750  3996622.75  ...   0.038821    0.198435   \n",
       "17977  0.32961 -0.238922   6152.000  3996622.75  ...   0.258249    0.000416   \n",
       "17978  0.32961  0.279242   2765.500  3996622.75  ...   0.039654   -0.348553   \n",
       "17979  0.32961 -0.159614   5268.500  3996622.75  ...  -0.438856   -0.478510   \n",
       "\n",
       "       Vloop_acf  Vloop_pacf    Time  Frame  Event  Label   Shot  Window  \n",
       "0       0.034098   -0.074084  10.361      0    0.0      0  81206       0  \n",
       "1       0.034098   -0.074084  10.362      1    0.0      0  81206       0  \n",
       "2       0.034098   -0.074084  10.363      2    0.0      0  81206       0  \n",
       "3       0.034098   -0.074084  10.364      3    0.0      0  81206       0  \n",
       "4       0.034098   -0.074084  10.365      4    0.0      0  81206       0  \n",
       "...          ...         ...     ...    ...    ...    ...    ...     ...  \n",
       "17975   0.039632    0.056425   8.622     15    0.0      0  98005     905  \n",
       "17976   0.039632    0.056425   8.623     16    0.0      0  98005     905  \n",
       "17977   0.039632    0.056425   8.624     17    0.0      0  98005     905  \n",
       "17978   0.039632    0.056425   8.625     18    0.0      0  98005     905  \n",
       "17979   0.039632    0.056425   8.626     19    0.0      0  98005     905  \n",
       "\n",
       "[17980 rows x 141 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels count:\n",
      "Label\n",
      "0    12840\n",
      "1     4140\n",
      "2     1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define columns to perform feature engineering on\n",
    "columns = ['IPLA', 'IPref', 'IPE', 'ECEcore', 'SSXcore', 'LI', 'Q95', 'ZMAG', 'Vloop']\n",
    "\n",
    "# Define the operations to perform on each column\n",
    "operations = {\n",
    "    'mean': np.mean,\n",
    "    'std': np.std,\n",
    "    'var': np.var,\n",
    "    'min': np.min,\n",
    "    'max': np.max,\n",
    "    'median': np.median,\n",
    "    'range': lambda x: np.max(x) - np.min(x),\n",
    "    'skew': pd.Series.skew,\n",
    "    'kurt': pd.Series.kurtosis,\n",
    "    'fft_abs': lambda x: np.abs(np.fft.fft(x)),\n",
    "    'der': np.gradient,\n",
    "    'der2': lambda x: np.gradient(np.gradient(x)),\n",
    "    'acf': lambda x: 0 if x.isna().any() or np.isinf(x).any() or x.nunique() <= 1 else acf(x).mean(),\n",
    "    'pacf': lambda x: 0 if x.isna().any() or np.isinf(x).any() or x.nunique() <= 1 else pacf(x).mean()\n",
    "}\n",
    "\n",
    "df_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each entry in the dataset\n",
    "for i, entry in enumerate(dataset):\n",
    "    # Extract data and label from the current entry\n",
    "    d = entry['x']\n",
    "    label = entry['y']\n",
    "    metadata = entry['metadata']\n",
    "    event = metadata['time_event']\n",
    "\n",
    "    # Create a DataFrame for the current entry\n",
    "    df = pd.DataFrame(d['data'], columns=d['columns'])\n",
    "\n",
    "    # Add the IPE column\n",
    "    df['IPE'] = np.abs(df['IPLA'] - df['IPref'])\n",
    "\n",
    "    # Perform feature engineering on each column\n",
    "    new_cols = {}\n",
    "    for col in columns:\n",
    "        for op_name, op_func in operations.items():\n",
    "            new_cols[col + '_' + op_name] = op_func(df[col])\n",
    "\n",
    "    # Create a new DataFrame with the new columns\n",
    "    df_new = pd.DataFrame(new_cols)\n",
    "\n",
    "    # Concatenate the original DataFrame with the new one\n",
    "    df = pd.concat([df, df_new], axis=1)\n",
    "    \n",
    "    # Add the time column\n",
    "    df['Time'] = d['time']\n",
    "\n",
    "    # Add the Frame, Event, Label, Shot and Window columns\n",
    "    df['Frame'] = range(0, 20)\n",
    "    df['Event'] = event if event else 0\n",
    "\n",
    "    if event:\n",
    "        # Find closest points to time_event\n",
    "        differences = np.abs(df['Time'] - event)\n",
    "        closest_indices = np.argsort(differences)[:20]\n",
    "\n",
    "        # Assign labels to closest points\n",
    "        df['Label'] = 0\n",
    "        df.loc[closest_indices, 'Label'] = label\n",
    "    else:\n",
    "        df['Label'] = label\n",
    "\n",
    "    df['Shot'] = metadata['shot']\n",
    "    df['Window'] = i  # Add the window number\n",
    "\n",
    "    contains_nan = df.isna().any().any()\n",
    "\n",
    "    if not contains_nan:\n",
    "        # Append the current DataFrame to the main DataFrame\n",
    "        df_data = pd.concat([df_data, df], ignore_index=True)\n",
    "\n",
    "df_data = df_data.dropna()\n",
    "display(df_data)\n",
    "print(\"Labels count:\")\n",
    "print(f\"{df_data['Label'].value_counts()}\")\n",
    "\n",
    "base_col = df_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6qCEtg82u00"
   },
   "source": [
    "We **normalize and split** into training and test sets before feeding it into our **Neural Network**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_qkjUMio62W",
    "outputId": "d02726ec-3769-4059-c375-a6cec7dad4ab"
   },
   "outputs": [],
   "source": [
    "# Split 'Shot' values into training and test sets and create DataFrames\n",
    "shot_train, shot_test = train_test_split(df_data['Shot'].unique(), test_size=0.2, random_state=42)\n",
    "train_df = df_data[df_data['Shot'].isin(shot_train)]\n",
    "test_df = df_data[df_data['Shot'].isin(shot_test)]\n",
    "\n",
    "# Group by 'Window' and 'Shot', and reshape the data\n",
    "train_df_grouped = train_df.sort_values(['Shot', 'Window', 'Time']).groupby(['Shot', 'Window'])\n",
    "test_df_grouped = test_df.sort_values(['Shot', 'Window', 'Time']).groupby(['Shot', 'Window'])\n",
    "\n",
    "# Prepare lists to hold sequences\n",
    "X_train_grouped, y_train_grouped = [], []\n",
    "X_test_grouped, y_test_grouped = [], []\n",
    "\n",
    "# Generate sequences for training data\n",
    "for _, group in train_df_grouped:\n",
    "    X_train_grouped.append(group.drop(columns=['Frame', 'Event', 'Label', 'Shot', 'Window']).values)\n",
    "    y_train_grouped.append(group['Label'].values[0])  # Assuming all labels in a group are the same\n",
    "\n",
    "# Generate sequences for testing data\n",
    "for _, group in test_df_grouped:\n",
    "    X_test_grouped.append(group.drop(columns=['Frame', 'Event', 'Label', 'Shot', 'Window']).values)\n",
    "    y_test_grouped.append(group['Label'].values[0])  # Assuming all labels in a group are the same\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train_grouped = np.array(X_train_grouped)\n",
    "y_train_grouped = np.array(y_train_grouped)\n",
    "X_test_grouped = np.array(X_test_grouped)\n",
    "y_test_grouped = np.array(y_test_grouped)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_grouped.reshape(-1, X_train_grouped.shape[-1])).reshape(X_train_grouped.shape)\n",
    "X_test = scaler.transform(X_test_grouped.reshape(-1, X_test_grouped.shape[-1])).reshape(X_test_grouped.shape)\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = to_categorical(y_train_grouped)\n",
    "y_test = to_categorical(y_test_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(737, 20, 136)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Z-zSjmr3ABP"
   },
   "source": [
    "#**II - Model tuning**\n",
    "\n",
    "Here we use the hyperparamter tuner from keras to find the best model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HngMK-Dgv6VR",
    "outputId": "cf3f47ac-938f-4711-8629-a6266d63c465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 20, 136)]            0         []                            \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 20, 64)               69696     ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 20, 64)               256       ['conv1d_1[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 20, 64)               0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 20, 64)               32832     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 20, 64)               256       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 20, 64)               0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 20, 64)               32832     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 20, 64)               256       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 20, 64)               0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 20, 64)               8768      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 20, 64)               0         ['activation_2[0][0]',        \n",
      "                                                                     'conv1d[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 20, 64)               20544     ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 20, 64)               256       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 20, 64)               0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 20, 64)               20544     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 20, 64)               256       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 20, 64)               0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 20, 64)               20544     ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 20, 64)               256       ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 20, 64)               0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 20, 64)               4160      ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 20, 64)               0         ['activation_5[0][0]',        \n",
      "                                                                     'conv1d_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 20, 64)               12352     ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 20, 64)               256       ['conv1d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 20, 64)               0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 20, 64)               12352     ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 20, 64)               256       ['conv1d_10[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 20, 64)               0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)          (None, 20, 64)               12352     ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 20, 64)               256       ['conv1d_11[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 20, 64)               0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 20, 64)               4160      ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 20, 64)               0         ['activation_8[0][0]',        \n",
      "                                                                     'conv1d_8[0][0]']            \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 64)                   0         ['add_2[0][0]']               \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 3)                    195       ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 253635 (990.76 KB)\n",
      "Trainable params: 252483 (986.26 KB)\n",
      "Non-trainable params: 1152 (4.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Add, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function for creating a residual block\n",
    "def residual_block(x, filters, kernel_size):\n",
    "    # Save the input value. Apply a 1x1 convolution to match the number of channels.\n",
    "    residual = Conv1D(filters, 1, padding='same')(x)\n",
    "\n",
    "    # Convolution, batch normalization, and ReLU activation (repeated three times)\n",
    "    for _ in range(3):\n",
    "        x = Conv1D(filters, kernel_size, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "    # Add the residual (input) to the output\n",
    "    x = Add()([x, residual])\n",
    "\n",
    "    return x\n",
    "\n",
    "# Input shape: (timesteps, features)\n",
    "input_shape = (20, 136)\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Create three residual blocks with varying kernel sizes\n",
    "x = residual_block(input_layer, 64, 8)\n",
    "x = residual_block(x, 64, 5)\n",
    "x = residual_block(x, 64, 3)\n",
    "\n",
    "# Global Average Pooling layer\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "# Final softmax classifier\n",
    "output_layer = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# Create the model and compile it\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = Adam(learning_rate=1e-7)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', tfa.metrics.F1Score(num_classes=3, average=None)])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "24/24 [==============================] - 6s 52ms/step - loss: 2.1367 - accuracy: 0.2646 - f1_score: 0.2609 - val_loss: 1.9465 - val_accuracy: 0.2901 - val_f1_score: 0.2861\n",
      "Epoch 2/10000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 2.0896 - accuracy: 0.2727 - f1_score: 0.2642 - val_loss: 1.8109 - val_accuracy: 0.3395 - val_f1_score: 0.3115\n",
      "Epoch 3/10000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 2.0986 - accuracy: 0.2673 - f1_score: 0.2630 - val_loss: 1.7509 - val_accuracy: 0.4012 - val_f1_score: 0.3340\n",
      "Epoch 4/10000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 2.1321 - accuracy: 0.2700 - f1_score: 0.2626 - val_loss: 1.7217 - val_accuracy: 0.3889 - val_f1_score: 0.3216\n",
      "Epoch 5/10000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 2.1331 - accuracy: 0.2659 - f1_score: 0.2591 - val_loss: 1.7329 - val_accuracy: 0.3704 - val_f1_score: 0.3112\n",
      "Epoch 6/10000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 2.1136 - accuracy: 0.2592 - f1_score: 0.2588 - val_loss: 1.7496 - val_accuracy: 0.3457 - val_f1_score: 0.2956\n",
      "Epoch 7/10000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 2.0874 - accuracy: 0.2564 - f1_score: 0.2547 - val_loss: 1.7677 - val_accuracy: 0.3272 - val_f1_score: 0.2887\n",
      "Epoch 8/10000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 2.1375 - accuracy: 0.2700 - f1_score: 0.2676 - val_loss: 1.7911 - val_accuracy: 0.3086 - val_f1_score: 0.2756\n",
      "Epoch 9/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 2.0926 - accuracy: 0.2619 - f1_score: 0.2560 - val_loss: 1.8155 - val_accuracy: 0.3086 - val_f1_score: 0.2814\n",
      "Epoch 10/10000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 2.0932 - accuracy: 0.2605 - f1_score: 0.2557 - val_loss: 1.8360 - val_accuracy: 0.3025 - val_f1_score: 0.2803\n",
      "Epoch 11/10000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 2.0704 - accuracy: 0.2836 - f1_score: 0.2727 - val_loss: 1.8520 - val_accuracy: 0.3025 - val_f1_score: 0.2802\n",
      "Epoch 12/10000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 2.0787 - accuracy: 0.2578 - f1_score: 0.2536 - val_loss: 1.8666 - val_accuracy: 0.2963 - val_f1_score: 0.2769\n",
      "Epoch 13/10000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 2.0481 - accuracy: 0.2768 - f1_score: 0.2713 - val_loss: 1.8770 - val_accuracy: 0.3025 - val_f1_score: 0.2802\n",
      "Epoch 14/10000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 2.0777 - accuracy: 0.2727 - f1_score: 0.2662 - val_loss: 1.8881 - val_accuracy: 0.3025 - val_f1_score: 0.2802\n",
      "Epoch 15/10000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 2.0496 - accuracy: 0.2659 - f1_score: 0.2583 - val_loss: 1.8981 - val_accuracy: 0.3025 - val_f1_score: 0.2802\n",
      "Epoch 16/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 2.0659 - accuracy: 0.2836 - f1_score: 0.2789 - val_loss: 1.9054 - val_accuracy: 0.3025 - val_f1_score: 0.2802\n",
      "Epoch 17/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 2.0665 - accuracy: 0.2659 - f1_score: 0.2622 - val_loss: 1.9152 - val_accuracy: 0.3086 - val_f1_score: 0.2854\n",
      "Epoch 18/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 2.0628 - accuracy: 0.2578 - f1_score: 0.2563 - val_loss: 1.9150 - val_accuracy: 0.3272 - val_f1_score: 0.2970\n",
      "Epoch 19/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 2.0690 - accuracy: 0.2578 - f1_score: 0.2512 - val_loss: 1.9200 - val_accuracy: 0.3272 - val_f1_score: 0.2970\n",
      "Epoch 20/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 2.0486 - accuracy: 0.2632 - f1_score: 0.2524 - val_loss: 1.9190 - val_accuracy: 0.3333 - val_f1_score: 0.3022\n",
      "Epoch 21/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 2.0455 - accuracy: 0.2741 - f1_score: 0.2666 - val_loss: 1.9169 - val_accuracy: 0.3333 - val_f1_score: 0.3022\n",
      "Epoch 22/10000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 2.0647 - accuracy: 0.2714 - f1_score: 0.2631 - val_loss: 1.9177 - val_accuracy: 0.3395 - val_f1_score: 0.3053\n",
      "Epoch 23/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 2.0559 - accuracy: 0.2809 - f1_score: 0.2754 - val_loss: 1.9184 - val_accuracy: 0.3395 - val_f1_score: 0.3053\n",
      "Epoch 24/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 2.0369 - accuracy: 0.2809 - f1_score: 0.2684 - val_loss: 1.9095 - val_accuracy: 0.3333 - val_f1_score: 0.3022\n",
      "Epoch 25/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 2.0424 - accuracy: 0.2768 - f1_score: 0.2659 - val_loss: 1.9065 - val_accuracy: 0.3395 - val_f1_score: 0.3053\n",
      "Epoch 26/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 2.0970 - accuracy: 0.2917 - f1_score: 0.2779 - val_loss: 1.9045 - val_accuracy: 0.3457 - val_f1_score: 0.3083\n",
      "Epoch 27/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 2.0156 - accuracy: 0.2754 - f1_score: 0.2684 - val_loss: 1.9068 - val_accuracy: 0.3395 - val_f1_score: 0.3053\n",
      "Epoch 28/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 2.0078 - accuracy: 0.2754 - f1_score: 0.2726 - val_loss: 1.9069 - val_accuracy: 0.3395 - val_f1_score: 0.3031\n",
      "Epoch 29/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 2.0630 - accuracy: 0.2822 - f1_score: 0.2797 - val_loss: 1.9051 - val_accuracy: 0.3395 - val_f1_score: 0.3031\n",
      "Epoch 30/10000\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 2.0319 - accuracy: 0.2714 - f1_score: 0.2684 - val_loss: 1.9060 - val_accuracy: 0.3333 - val_f1_score: 0.3001\n",
      "Epoch 31/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 2.0303 - accuracy: 0.2768 - f1_score: 0.2695 - val_loss: 1.9070 - val_accuracy: 0.3272 - val_f1_score: 0.2970\n",
      "Epoch 32/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 2.0192 - accuracy: 0.2822 - f1_score: 0.2757 - val_loss: 1.9051 - val_accuracy: 0.3272 - val_f1_score: 0.2970\n",
      "Epoch 33/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 2.0595 - accuracy: 0.2877 - f1_score: 0.2841 - val_loss: 1.9005 - val_accuracy: 0.3333 - val_f1_score: 0.3001\n",
      "Epoch 34/10000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 1.9902 - accuracy: 0.2849 - f1_score: 0.2769 - val_loss: 1.8944 - val_accuracy: 0.3333 - val_f1_score: 0.3001\n",
      "Epoch 35/10000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 2.0429 - accuracy: 0.2795 - f1_score: 0.2713 - val_loss: 1.8933 - val_accuracy: 0.3333 - val_f1_score: 0.3001\n",
      "Epoch 36/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 2.0326 - accuracy: 0.2782 - f1_score: 0.2713 - val_loss: 1.8908 - val_accuracy: 0.3457 - val_f1_score: 0.3105\n",
      "Epoch 37/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 2.0065 - accuracy: 0.2863 - f1_score: 0.2786 - val_loss: 1.8912 - val_accuracy: 0.3395 - val_f1_score: 0.3053\n",
      "Epoch 38/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 2.0124 - accuracy: 0.2958 - f1_score: 0.2886 - val_loss: 1.8865 - val_accuracy: 0.3457 - val_f1_score: 0.3105\n",
      "Epoch 39/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 2.0058 - accuracy: 0.2877 - f1_score: 0.2787 - val_loss: 1.8867 - val_accuracy: 0.3333 - val_f1_score: 0.3022\n",
      "Epoch 40/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 2.0026 - accuracy: 0.2849 - f1_score: 0.2774 - val_loss: 1.8849 - val_accuracy: 0.3272 - val_f1_score: 0.2970\n",
      "Epoch 41/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 2.0561 - accuracy: 0.2849 - f1_score: 0.2817 - val_loss: 1.8823 - val_accuracy: 0.3395 - val_f1_score: 0.3075\n",
      "Epoch 42/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 2.0009 - accuracy: 0.2795 - f1_score: 0.2759 - val_loss: 1.8799 - val_accuracy: 0.3395 - val_f1_score: 0.3075\n",
      "Epoch 43/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 1.9834 - accuracy: 0.2904 - f1_score: 0.2805 - val_loss: 1.8762 - val_accuracy: 0.3519 - val_f1_score: 0.3136\n",
      "Epoch 44/10000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 1.9679 - accuracy: 0.2877 - f1_score: 0.2775 - val_loss: 1.8730 - val_accuracy: 0.3457 - val_f1_score: 0.3083\n",
      "Epoch 45/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.9862 - accuracy: 0.2863 - f1_score: 0.2753 - val_loss: 1.8774 - val_accuracy: 0.3519 - val_f1_score: 0.3136\n",
      "Epoch 46/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.9943 - accuracy: 0.2809 - f1_score: 0.2698 - val_loss: 1.8732 - val_accuracy: 0.3333 - val_f1_score: 0.3001\n",
      "Epoch 47/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.9682 - accuracy: 0.2890 - f1_score: 0.2793 - val_loss: 1.8713 - val_accuracy: 0.3395 - val_f1_score: 0.3031\n",
      "Epoch 48/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 1.9568 - accuracy: 0.2890 - f1_score: 0.2784 - val_loss: 1.8692 - val_accuracy: 0.3457 - val_f1_score: 0.3126\n",
      "Epoch 49/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.9686 - accuracy: 0.3012 - f1_score: 0.2859 - val_loss: 1.8676 - val_accuracy: 0.3519 - val_f1_score: 0.3156\n",
      "Epoch 50/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.9449 - accuracy: 0.2944 - f1_score: 0.2797 - val_loss: 1.8713 - val_accuracy: 0.3457 - val_f1_score: 0.3062\n",
      "Epoch 51/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.9578 - accuracy: 0.2944 - f1_score: 0.2881 - val_loss: 1.8648 - val_accuracy: 0.3519 - val_f1_score: 0.3136\n",
      "Epoch 52/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 1.9632 - accuracy: 0.2659 - f1_score: 0.2619 - val_loss: 1.8639 - val_accuracy: 0.3457 - val_f1_score: 0.3062\n",
      "Epoch 53/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.9630 - accuracy: 0.3053 - f1_score: 0.2911 - val_loss: 1.8624 - val_accuracy: 0.3457 - val_f1_score: 0.3104\n",
      "Epoch 54/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.9682 - accuracy: 0.2768 - f1_score: 0.2727 - val_loss: 1.8608 - val_accuracy: 0.3580 - val_f1_score: 0.3186\n",
      "Epoch 55/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.9328 - accuracy: 0.2822 - f1_score: 0.2710 - val_loss: 1.8520 - val_accuracy: 0.3580 - val_f1_score: 0.3186\n",
      "Epoch 56/10000\n",
      "24/24 [==============================] - 1s 20ms/step - loss: 1.9655 - accuracy: 0.2917 - f1_score: 0.2863 - val_loss: 1.8381 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 57/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.9435 - accuracy: 0.2917 - f1_score: 0.2809 - val_loss: 1.8374 - val_accuracy: 0.3642 - val_f1_score: 0.3239\n",
      "Epoch 58/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.9510 - accuracy: 0.2985 - f1_score: 0.2879 - val_loss: 1.8381 - val_accuracy: 0.3642 - val_f1_score: 0.3239\n",
      "Epoch 59/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.9591 - accuracy: 0.3012 - f1_score: 0.2873 - val_loss: 1.8390 - val_accuracy: 0.3580 - val_f1_score: 0.3186\n",
      "Epoch 60/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.9621 - accuracy: 0.2863 - f1_score: 0.2767 - val_loss: 1.8333 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 61/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 1.9552 - accuracy: 0.2836 - f1_score: 0.2763 - val_loss: 1.8325 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 62/10000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 1.9480 - accuracy: 0.2999 - f1_score: 0.2915 - val_loss: 1.8342 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 63/10000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 1.9111 - accuracy: 0.3080 - f1_score: 0.3002 - val_loss: 1.8310 - val_accuracy: 0.3642 - val_f1_score: 0.3239\n",
      "Epoch 64/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.9136 - accuracy: 0.2931 - f1_score: 0.2826 - val_loss: 1.8300 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 65/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.9393 - accuracy: 0.3202 - f1_score: 0.3052 - val_loss: 1.8285 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 66/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 1.8882 - accuracy: 0.3161 - f1_score: 0.3015 - val_loss: 1.8290 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 67/10000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 1.9448 - accuracy: 0.2958 - f1_score: 0.2883 - val_loss: 1.8265 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 68/10000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 1.9104 - accuracy: 0.3053 - f1_score: 0.2908 - val_loss: 1.8206 - val_accuracy: 0.3642 - val_f1_score: 0.3216\n",
      "Epoch 69/10000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 1.9020 - accuracy: 0.2931 - f1_score: 0.2813 - val_loss: 1.8137 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 70/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8736 - accuracy: 0.3297 - f1_score: 0.3153 - val_loss: 1.8108 - val_accuracy: 0.3642 - val_f1_score: 0.3216\n",
      "Epoch 71/10000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 1.9152 - accuracy: 0.3107 - f1_score: 0.2980 - val_loss: 1.8101 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 72/10000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 1.9133 - accuracy: 0.2931 - f1_score: 0.2866 - val_loss: 1.8133 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 73/10000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 1.8682 - accuracy: 0.3270 - f1_score: 0.3142 - val_loss: 1.8047 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 74/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.9036 - accuracy: 0.3121 - f1_score: 0.2991 - val_loss: 1.7929 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 75/10000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 1.8888 - accuracy: 0.3175 - f1_score: 0.3062 - val_loss: 1.7937 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 76/10000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 1.8884 - accuracy: 0.3134 - f1_score: 0.3016 - val_loss: 1.7905 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 77/10000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 1.8676 - accuracy: 0.3148 - f1_score: 0.2979 - val_loss: 1.7884 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 78/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8876 - accuracy: 0.3107 - f1_score: 0.2984 - val_loss: 1.7903 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 79/10000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 1.8990 - accuracy: 0.3080 - f1_score: 0.2969 - val_loss: 1.7952 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 80/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8823 - accuracy: 0.3012 - f1_score: 0.2959 - val_loss: 1.7908 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 81/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.9044 - accuracy: 0.3026 - f1_score: 0.2903 - val_loss: 1.7875 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 82/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8909 - accuracy: 0.3066 - f1_score: 0.2950 - val_loss: 1.7812 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 83/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8718 - accuracy: 0.2985 - f1_score: 0.2915 - val_loss: 1.7847 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 84/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 1.8813 - accuracy: 0.3080 - f1_score: 0.2992 - val_loss: 1.7787 - val_accuracy: 0.3765 - val_f1_score: 0.3298\n",
      "Epoch 85/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8382 - accuracy: 0.3256 - f1_score: 0.3076 - val_loss: 1.7719 - val_accuracy: 0.3765 - val_f1_score: 0.3298\n",
      "Epoch 86/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.8802 - accuracy: 0.3161 - f1_score: 0.3021 - val_loss: 1.7762 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 87/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.8855 - accuracy: 0.3053 - f1_score: 0.2901 - val_loss: 1.7781 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 88/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.8628 - accuracy: 0.3148 - f1_score: 0.3041 - val_loss: 1.7823 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 89/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 1.8592 - accuracy: 0.3243 - f1_score: 0.3007 - val_loss: 1.7784 - val_accuracy: 0.3765 - val_f1_score: 0.3298\n",
      "Epoch 90/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.8702 - accuracy: 0.3256 - f1_score: 0.3151 - val_loss: 1.7753 - val_accuracy: 0.3765 - val_f1_score: 0.3298\n",
      "Epoch 91/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8710 - accuracy: 0.3080 - f1_score: 0.2979 - val_loss: 1.7666 - val_accuracy: 0.3765 - val_f1_score: 0.3298\n",
      "Epoch 92/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.8886 - accuracy: 0.3175 - f1_score: 0.3047 - val_loss: 1.7582 - val_accuracy: 0.3765 - val_f1_score: 0.3298\n",
      "Epoch 93/10000\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 1.8430 - accuracy: 0.3175 - f1_score: 0.3036 - val_loss: 1.7631 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 94/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8891 - accuracy: 0.3066 - f1_score: 0.2967 - val_loss: 1.7607 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 95/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.8684 - accuracy: 0.3189 - f1_score: 0.3030 - val_loss: 1.7541 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 96/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8369 - accuracy: 0.3148 - f1_score: 0.3024 - val_loss: 1.7537 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 97/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8269 - accuracy: 0.3243 - f1_score: 0.3091 - val_loss: 1.7515 - val_accuracy: 0.3765 - val_f1_score: 0.3321\n",
      "Epoch 98/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.8163 - accuracy: 0.3148 - f1_score: 0.3038 - val_loss: 1.7509 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 99/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8223 - accuracy: 0.3148 - f1_score: 0.3009 - val_loss: 1.7492 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 100/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8346 - accuracy: 0.3284 - f1_score: 0.3093 - val_loss: 1.7463 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 101/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.8273 - accuracy: 0.3134 - f1_score: 0.3033 - val_loss: 1.7486 - val_accuracy: 0.3704 - val_f1_score: 0.3268\n",
      "Epoch 102/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.7955 - accuracy: 0.3270 - f1_score: 0.3116 - val_loss: 1.7492 - val_accuracy: 0.3889 - val_f1_score: 0.3495\n",
      "Epoch 103/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8120 - accuracy: 0.3297 - f1_score: 0.3163 - val_loss: 1.7466 - val_accuracy: 0.3889 - val_f1_score: 0.3495\n",
      "Epoch 104/10000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 1.8325 - accuracy: 0.3121 - f1_score: 0.2986 - val_loss: 1.7488 - val_accuracy: 0.3889 - val_f1_score: 0.3495\n",
      "Epoch 105/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8200 - accuracy: 0.3284 - f1_score: 0.3156 - val_loss: 1.7416 - val_accuracy: 0.3951 - val_f1_score: 0.3549\n",
      "Epoch 106/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8286 - accuracy: 0.3148 - f1_score: 0.3055 - val_loss: 1.7429 - val_accuracy: 0.3889 - val_f1_score: 0.3495\n",
      "Epoch 107/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.7932 - accuracy: 0.3433 - f1_score: 0.3286 - val_loss: 1.7407 - val_accuracy: 0.3951 - val_f1_score: 0.3549\n",
      "Epoch 108/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 1.8023 - accuracy: 0.3229 - f1_score: 0.3095 - val_loss: 1.7345 - val_accuracy: 0.3951 - val_f1_score: 0.3526\n",
      "Epoch 109/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8117 - accuracy: 0.3229 - f1_score: 0.3061 - val_loss: 1.7312 - val_accuracy: 0.4012 - val_f1_score: 0.3579\n",
      "Epoch 110/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8105 - accuracy: 0.3202 - f1_score: 0.3056 - val_loss: 1.7356 - val_accuracy: 0.3951 - val_f1_score: 0.3549\n",
      "Epoch 111/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.7802 - accuracy: 0.3379 - f1_score: 0.3222 - val_loss: 1.7322 - val_accuracy: 0.3951 - val_f1_score: 0.3549\n",
      "Epoch 112/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8127 - accuracy: 0.3392 - f1_score: 0.3238 - val_loss: 1.7267 - val_accuracy: 0.3951 - val_f1_score: 0.3526\n",
      "Epoch 113/10000\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 1.8131 - accuracy: 0.3256 - f1_score: 0.3114 - val_loss: 1.7256 - val_accuracy: 0.4012 - val_f1_score: 0.3579\n",
      "Epoch 114/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.7887 - accuracy: 0.3243 - f1_score: 0.3128 - val_loss: 1.7246 - val_accuracy: 0.4012 - val_f1_score: 0.3579\n",
      "Epoch 115/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8139 - accuracy: 0.3107 - f1_score: 0.2991 - val_loss: 1.7161 - val_accuracy: 0.4012 - val_f1_score: 0.3579\n",
      "Epoch 116/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.7932 - accuracy: 0.3324 - f1_score: 0.3181 - val_loss: 1.7177 - val_accuracy: 0.3951 - val_f1_score: 0.3549\n",
      "Epoch 117/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.7937 - accuracy: 0.3338 - f1_score: 0.3152 - val_loss: 1.7143 - val_accuracy: 0.3951 - val_f1_score: 0.3549\n",
      "Epoch 118/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.7572 - accuracy: 0.3338 - f1_score: 0.3185 - val_loss: 1.7119 - val_accuracy: 0.4074 - val_f1_score: 0.3684\n",
      "Epoch 119/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 1.7606 - accuracy: 0.3446 - f1_score: 0.3270 - val_loss: 1.7117 - val_accuracy: 0.3951 - val_f1_score: 0.3598\n",
      "Epoch 120/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.8298 - accuracy: 0.3202 - f1_score: 0.3111 - val_loss: 1.7044 - val_accuracy: 0.4012 - val_f1_score: 0.3579\n",
      "Epoch 121/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.7701 - accuracy: 0.3365 - f1_score: 0.3243 - val_loss: 1.7026 - val_accuracy: 0.4074 - val_f1_score: 0.3609\n",
      "Epoch 122/10000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 1.7534 - accuracy: 0.3446 - f1_score: 0.3270 - val_loss: 1.6998 - val_accuracy: 0.3951 - val_f1_score: 0.3549\n",
      "Epoch 123/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.7864 - accuracy: 0.3324 - f1_score: 0.3154 - val_loss: 1.7006 - val_accuracy: 0.3951 - val_f1_score: 0.3549\n",
      "Epoch 124/10000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 1.7406 - accuracy: 0.3379 - f1_score: 0.3176 - val_loss: 1.6921 - val_accuracy: 0.4074 - val_f1_score: 0.3634\n",
      "Epoch 125/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.7739 - accuracy: 0.3216 - f1_score: 0.3060 - val_loss: 1.6888 - val_accuracy: 0.4074 - val_f1_score: 0.3634\n",
      "Epoch 126/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.7598 - accuracy: 0.3406 - f1_score: 0.3232 - val_loss: 1.6864 - val_accuracy: 0.4012 - val_f1_score: 0.3604\n",
      "Epoch 127/10000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 1.7629 - accuracy: 0.3338 - f1_score: 0.3190 - val_loss: 1.6713 - val_accuracy: 0.4136 - val_f1_score: 0.3664\n",
      "Epoch 128/10000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 1.7472 - accuracy: 0.3528 - f1_score: 0.3358 - val_loss: 1.6688 - val_accuracy: 0.4136 - val_f1_score: 0.3664\n",
      "Epoch 129/10000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 1.7718 - accuracy: 0.3243 - f1_score: 0.3061 - val_loss: 1.6693 - val_accuracy: 0.4136 - val_f1_score: 0.3664\n",
      "Epoch 130/10000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 1.7308 - accuracy: 0.3365 - f1_score: 0.3188 - val_loss: 1.6708 - val_accuracy: 0.4198 - val_f1_score: 0.3693\n",
      "Epoch 131/10000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 1.8100 - accuracy: 0.3324 - f1_score: 0.3181 - val_loss: 1.6682 - val_accuracy: 0.4198 - val_f1_score: 0.3693\n",
      "Epoch 132/10000\n",
      "24/24 [==============================] - 1s 20ms/step - loss: 1.7403 - accuracy: 0.3243 - f1_score: 0.3114 - val_loss: 1.6483 - val_accuracy: 0.4444 - val_f1_score: 0.3887\n",
      "Epoch 133/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.7327 - accuracy: 0.3528 - f1_score: 0.3372 - val_loss: 1.6619 - val_accuracy: 0.4259 - val_f1_score: 0.3699\n",
      "Epoch 134/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.7805 - accuracy: 0.3365 - f1_score: 0.3242 - val_loss: 1.6628 - val_accuracy: 0.4383 - val_f1_score: 0.3781\n",
      "Epoch 135/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.7005 - accuracy: 0.3460 - f1_score: 0.3288 - val_loss: 1.6659 - val_accuracy: 0.4444 - val_f1_score: 0.3810\n",
      "Epoch 136/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.6962 - accuracy: 0.3433 - f1_score: 0.3281 - val_loss: 1.6609 - val_accuracy: 0.4383 - val_f1_score: 0.3781\n",
      "Epoch 137/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.7269 - accuracy: 0.3623 - f1_score: 0.3464 - val_loss: 1.6658 - val_accuracy: 0.4321 - val_f1_score: 0.3752\n",
      "Epoch 138/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.7491 - accuracy: 0.3474 - f1_score: 0.3378 - val_loss: 1.6670 - val_accuracy: 0.4259 - val_f1_score: 0.3723\n",
      "Epoch 139/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.7118 - accuracy: 0.3582 - f1_score: 0.3367 - val_loss: 1.6659 - val_accuracy: 0.4259 - val_f1_score: 0.3723\n",
      "Epoch 140/10000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 1.7171 - accuracy: 0.3474 - f1_score: 0.3282 - val_loss: 1.6614 - val_accuracy: 0.4259 - val_f1_score: 0.3723\n",
      "Epoch 141/10000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 1.7326 - accuracy: 0.3623 - f1_score: 0.3471 - val_loss: 1.6581 - val_accuracy: 0.4321 - val_f1_score: 0.3752\n",
      "Epoch 142/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.7337 - accuracy: 0.3555 - f1_score: 0.3450 - val_loss: 1.6630 - val_accuracy: 0.4259 - val_f1_score: 0.3723\n",
      "Epoch 143/10000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 1.7046 - accuracy: 0.3406 - f1_score: 0.3226 - val_loss: 1.6607 - val_accuracy: 0.4259 - val_f1_score: 0.3723\n",
      "Epoch 144/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.7122 - accuracy: 0.3555 - f1_score: 0.3362 - val_loss: 1.6630 - val_accuracy: 0.4321 - val_f1_score: 0.3829\n",
      "Epoch 145/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 1.7182 - accuracy: 0.3406 - f1_score: 0.3229 - val_loss: 1.6577 - val_accuracy: 0.4321 - val_f1_score: 0.3829\n",
      "Epoch 146/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.6921 - accuracy: 0.3474 - f1_score: 0.3299 - val_loss: 1.6537 - val_accuracy: 0.4321 - val_f1_score: 0.3752\n",
      "Epoch 147/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.7522 - accuracy: 0.3501 - f1_score: 0.3251 - val_loss: 1.6517 - val_accuracy: 0.4383 - val_f1_score: 0.3858\n",
      "Epoch 148/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.7663 - accuracy: 0.3311 - f1_score: 0.3111 - val_loss: 1.6529 - val_accuracy: 0.4383 - val_f1_score: 0.3858\n",
      "Epoch 149/10000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 1.6828 - accuracy: 0.3569 - f1_score: 0.3337 - val_loss: 1.6458 - val_accuracy: 0.4383 - val_f1_score: 0.3858\n",
      "Epoch 150/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.6670 - accuracy: 0.3514 - f1_score: 0.3313 - val_loss: 1.6169 - val_accuracy: 0.4506 - val_f1_score: 0.3839\n",
      "Epoch 151/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 1.6896 - accuracy: 0.3487 - f1_score: 0.3265 - val_loss: 1.6237 - val_accuracy: 0.4506 - val_f1_score: 0.3839\n",
      "Epoch 152/10000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 1.6893 - accuracy: 0.3419 - f1_score: 0.3252 - val_loss: 1.6228 - val_accuracy: 0.4568 - val_f1_score: 0.3945\n",
      "Epoch 153/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.6971 - accuracy: 0.3582 - f1_score: 0.3424 - val_loss: 1.6307 - val_accuracy: 0.4506 - val_f1_score: 0.3916\n",
      "Epoch 154/10000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 1.7081 - accuracy: 0.3338 - f1_score: 0.3140 - val_loss: 1.6260 - val_accuracy: 0.4444 - val_f1_score: 0.3810\n",
      "Epoch 155/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.6743 - accuracy: 0.3311 - f1_score: 0.3137 - val_loss: 1.6328 - val_accuracy: 0.4444 - val_f1_score: 0.3887\n",
      "Epoch 156/10000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.7249 - accuracy: 0.3460 - f1_score: 0.3316 - val_loss: 1.6333 - val_accuracy: 0.4506 - val_f1_score: 0.3916\n",
      "Epoch 157/10000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.6924 - accuracy: 0.3514 - f1_score: 0.3323 - val_loss: 1.6306 - val_accuracy: 0.4506 - val_f1_score: 0.3916\n",
      "Epoch 158/10000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 1.6673 - accuracy: 0.3541 - f1_score: 0.3363 - val_loss: 1.6213 - val_accuracy: 0.4506 - val_f1_score: 0.3916\n",
      "Epoch 159/10000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 1.6584 - accuracy: 0.3596 - f1_score: 0.3478 - val_loss: 1.6210 - val_accuracy: 0.4506 - val_f1_score: 0.3916\n",
      "Epoch 160/10000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 1.7099 - accuracy: 0.3297 - f1_score: 0.3159 - val_loss: 1.6313 - val_accuracy: 0.4506 - val_f1_score: 0.3916\n",
      "Epoch 161/10000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 1.7231 - accuracy: 0.3433 - f1_score: 0.3316 - val_loss: 1.6289 - val_accuracy: 0.4506 - val_f1_score: 0.3916\n",
      "Epoch 162/10000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 1.6727 - accuracy: 0.3528 - f1_score: 0.3342 - val_loss: 1.6299 - val_accuracy: 0.4444 - val_f1_score: 0.3810\n",
      "Epoch 163/10000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 1.6612 - accuracy: 0.3582 - f1_score: 0.3384 - val_loss: 1.6320 - val_accuracy: 0.4444 - val_f1_score: 0.3810\n",
      "Epoch 164/10000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 1.6718 - accuracy: 0.3636 - f1_score: 0.3375 - val_loss: 1.6340 - val_accuracy: 0.4383 - val_f1_score: 0.3781\n",
      "Epoch 165/10000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 1.6772 - accuracy: 0.3474 - f1_score: 0.3322 - val_loss: 1.6291 - val_accuracy: 0.4444 - val_f1_score: 0.3887\n",
      "Epoch 166/10000\n",
      " 8/24 [=========>....................] - ETA: 0s - loss: 1.7162 - accuracy: 0.3711 - f1_score: 0.3591"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10000, validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot the losses\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the F1-score on the validation set\n",
    "_, _, f1_score = model.evaluate(X_test, y_test)\n",
    "print('F1-score on validation set:', f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**III - Hyper-parameter tuning**\n",
    "\n",
    "We use keras tuner to find the best hyper-parameters for our Residual Network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 01m 27s]\n",
      "val_loss: 0.23076678812503815\n",
      "\n",
      "Best val_loss So Far: 0.13399605453014374\n",
      "Total elapsed time: 00h 15m 57s\n",
      "Best Hyperparameters:\n",
      "{'filters_0': 64, 'kernel_size_0': 5, 'filters_1': 96, 'kernel_size_1': 3, 'filters_2': 64, 'kernel_size_2': 13}\n"
     ]
    }
   ],
   "source": [
    "# Define the function to build the model with hyperparameters\n",
    "def build_model(hp):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    for i in range(3):\n",
    "        # Define hyperparameters for the number of filters and kernel size\n",
    "        filters = hp.Int('filters_' + str(i), min_value=32, max_value=128, step=32)\n",
    "        kernel_size = hp.Choice('kernel_size_' + str(i), values=[3, 5, 8, 13, 21])\n",
    "\n",
    "        x = residual_block(x, filters, kernel_size)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    # Final softmax classifier\n",
    "    output_layer = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Define hyperparameters for the learning rate\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', tfa.metrics.F1Score(num_classes=3, average=None)]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,  # Number of hyperparameter combinations to try\n",
    "    executions_per_trial=1,  # Number of models to train for each trial\n",
    "    directory='my_dir0',  # Directory where the hyperparameters will be saved\n",
    "    project_name='resnet_hyperparam_tuning'\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 6ms/step - loss: 0.1340 - accuracy: 0.9568 - f1_score: 0.9225\n",
      "F1-score on validation set:\n",
      "[0.97692305 0.85714287 0.93333334]\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "Weighted F1-score on validation set:\n",
      "0.9565549676660788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate the best model\n",
    "_, _, f1_score = best_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the F1-score on the validation set for each class\n",
    "print('F1-score on validation set:')\n",
    "print(f1_score)\n",
    "\n",
    "# Predict the classes\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Convert the predictions and true values to their original classes if they are one-hot encoded\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Get the classification report\n",
    "report = classification_report(y_test_classes, y_pred_classes, output_dict=True)\n",
    "\n",
    "# The weighted F1-score is available in the 'weighted avg' key in the report\n",
    "weighted_f1_score = report['weighted avg']['f1-score']\n",
    "\n",
    "print('Weighted F1-score on validation set:')\n",
    "print(weighted_f1_score)\n",
    "\n",
    "# Save the model\n",
    "best_model.save(filepath='../models/resnet_boosted.keras', overwrite=True, save_format='keras')\n",
    "\n",
    "# Save the model's weights\n",
    "best_model.save_weights('../models/resnet_boosted_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
